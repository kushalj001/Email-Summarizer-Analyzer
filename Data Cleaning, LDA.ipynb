{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning and Preprocessing\n",
    "**Author** - Kushal "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Understanding data\n",
    "* Following few cells are about understanding the data, the email text,languages and other parts that make it up. \n",
    "* First of all the data has a few duplicate values, i.e some emails are repeated more than once.\n",
    "* Also, many emails have multiple languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Information</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ciao Maurizio,\\n\\nGrazie mille per aver trovat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ciao Maurizio,\\n\\nGrazie mille per aver trovat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>こんにちは、 のユーザー様。\\n\\nこの問題についてご連絡いただきありがとうございます。 申...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>こんにちは、 のユーザー様。\\n\\nこの問題についてご連絡いただきありがとうございます。 申...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hi Stephane,\\n\\nThank you for keeping me updat...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Information\n",
       "0  Ciao Maurizio,\\n\\nGrazie mille per aver trovat...\n",
       "1  Ciao Maurizio,\\n\\nGrazie mille per aver trovat...\n",
       "2  こんにちは、 のユーザー様。\\n\\nこの問題についてご連絡いただきありがとうございます。 申...\n",
       "3  こんにちは、 のユーザー様。\\n\\nこの問題についてご連絡いただきありがとうございます。 申...\n",
       "4  Hi Stephane,\\n\\nThank you for keeping me updat..."
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_excel('sampledata.xlsx')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hi Stephane,',\n",
       " '',\n",
       " \"Thank you for keeping me updated on this issue. I'm happy to hear that the issue got resolved after all and you can now use   in its full functionality again. \",\n",
       " 'Also many thanks for your suggestions. For cards that are mainly using (for example) QR codes, the scanning example will also adapt to this format.',\n",
       " 'We hope to improve this feature for all cards in the future. ',\n",
       " '',\n",
       " \"In case you experience any further problems with your   app, please don't hesitate to contact me again.\",\n",
       " '',\n",
       " 'Best regards,',\n",
       " '',\n",
       " '',\n",
       " 'Solveig Miriam Brandt',\n",
       " 'Customer Support',\n",
       " '',\n",
       " '  GmbH',\n",
       " 'C-HUB / Hafenstraße 25-27',\n",
       " '68159 Mannheim']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Understanding the structure of emails\n",
    "\n",
    "x = data['Information'][4]\n",
    "x.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Emails:  596\n",
      "Unique Emails:  Information    492\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# There are duplicate emails in the dataset. \n",
    "\n",
    "print('Total Emails: ',len(data))\n",
    "print('Unique Emails: ',data.nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "####  Removing Salutations\n",
    "* The following method treats the emails irrespective of the language that it contains. Since the salutation, address and  other email meta-data is unlikely to provide any useful information when it comes to summarization, I have simply omitted the sentences based on their length.\n",
    "* Sentences of smaller size correspond to the salutions (in the beginning) and other details in the end. Morover larger sentences are always a part of the email body or payload."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_salutation():\n",
    "    '''\n",
    "       Removes salutation and other information not required for summarization by simply\n",
    "       considering the length of the sentence.\n",
    "    '''\n",
    "    \n",
    "    emails = list(data['Information'])\n",
    "    other_info = []\n",
    "    email_text = []\n",
    "    \n",
    "    for sample in emails:\n",
    "        #print(sample)\n",
    "        #print('---------------------------------------------------------------------')\n",
    "        sample = sample.split('\\n')\n",
    "        n = len(sample)\n",
    "        text = []\n",
    "        info = []\n",
    "        for i in range(1,n-1):\n",
    "            if len(sample[i])>50:\n",
    "                text.append(sample[i])\n",
    "            elif sample[i] != '':\n",
    "                info.append(sample[i])\n",
    "                \n",
    "        info_text = '---'.join([x for x in info])        \n",
    "        str_text = ''.join([x for x in text])\n",
    "        \n",
    "        other_info.append(info_text)\n",
    "        email_text.append(str_text)\n",
    "    \n",
    "    return email_text,other_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "emails,other_info = remove_salutation()\n",
    "email_set = list(set(emails))\n",
    "info_set = list(set(other_info))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Detecting Languages \n",
    "\n",
    "* Following cells make use of two libraries, langdetect to detect languages in the text and iso639 to map the ISO language codes to their actual name.\n",
    "* langdetect uses a non-deterministic algorithm to detect langauges and hence can be ambiguous.\n",
    "* 9 languages are detected in the whole dataset, with the majority of them being english. For further data cleaning tasks, I  have considered only top 6 langauges for convenience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langdetect import detect,detect_langs\n",
    "from iso639 import languages\n",
    "from langdetect import DetectorFactory\n",
    "\n",
    "DetectorFactory.seed = 0 # to deal with non-determinism\n",
    "\n",
    "def get_all_languages():\n",
    "    '''Gets all the languages used in the dataset.'''\n",
    "    \n",
    "    langs = []\n",
    "    lang_names = []\n",
    "    for email in email_set:\n",
    "        lang = detect(email)\n",
    "        if lang not in langs:\n",
    "            langs.append(lang)\n",
    "            name = languages.get(alpha2=lang).name\n",
    "            lang_names.append(name)\n",
    "\n",
    "  \n",
    "    return lang_names,langs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['en', 'de', 'it', 'nl', 'fr', 'es', 'ja', 'ru', 'pl']\n",
      "['English', 'German', 'Italian', 'Dutch', 'French', 'Spanish', 'Japanese', 'Russian', 'Polish']\n"
     ]
    }
   ],
   "source": [
    "lang_names,langs = get_all_languages()\n",
    "print(langs)\n",
    "print(lang_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Cleaned Emails</th>\n",
       "      <th>Other Info</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Grazie mille per aver trovato il tempo per met...</td>\n",
       "      <td>Tanti saluti,---Isabelle van Capelleveen---Cus...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Grazie mille per aver trovato il tempo per met...</td>\n",
       "      <td>Tanti saluti,---Isabelle van Capelleveen---Cus...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>この問題についてご連絡いただきありがとうございます。 申し訳ありませんが私は日本語語が話せま...</td>\n",
       "      <td>その他にもご質問や改善の提案、一般的なご意見などございましたら、お気軽にお問い合わせください...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>この問題についてご連絡いただきありがとうございます。 申し訳ありませんが私は日本語語が話せま...</td>\n",
       "      <td>その他にもご質問や改善の提案、一般的なご意見などございましたら、お気軽にお問い合わせください...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Thank you for keeping me updated on this issue...</td>\n",
       "      <td>Best regards,---Solveig Miriam Brandt---Custom...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      Cleaned Emails  \\\n",
       "0  Grazie mille per aver trovato il tempo per met...   \n",
       "1  Grazie mille per aver trovato il tempo per met...   \n",
       "2  この問題についてご連絡いただきありがとうございます。 申し訳ありませんが私は日本語語が話せま...   \n",
       "3  この問題についてご連絡いただきありがとうございます。 申し訳ありませんが私は日本語語が話せま...   \n",
       "4  Thank you for keeping me updated on this issue...   \n",
       "\n",
       "                                          Other Info  \n",
       "0  Tanti saluti,---Isabelle van Capelleveen---Cus...  \n",
       "1  Tanti saluti,---Isabelle van Capelleveen---Cus...  \n",
       "2  その他にもご質問や改善の提案、一般的なご意見などございましたら、お気軽にお問い合わせください...  \n",
       "3  その他にもご質問や改善の提案、一般的なご意見などございましたら、お気軽にお問い合わせください...  \n",
       "4  Best regards,---Solveig Miriam Brandt---Custom...  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "email_df = pd.DataFrame({'Cleaned Emails':emails,'Other Info':other_info})\n",
    "email_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lang(row):\n",
    "    text = row['Cleaned Emails']\n",
    "    lang = detect(text)\n",
    "    return lang    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "en    474\n",
       "de     33\n",
       "it     31\n",
       "nl     22\n",
       "fr     20\n",
       "es     11\n",
       "ru      3\n",
       "pl      1\n",
       "ja      1\n",
       "Name: Language, dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Most of the emails are in english\n",
    "\n",
    "email_df['Language'] = email_df.apply(get_lang,axis=1)\n",
    "email_df['Language'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_df = email_df[email_df['Language'] == 'en']\n",
    "#email_df.to_pickle('./new_df')\n",
    "#eng_df.to_pickle('./eng_df')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming\n",
    "* Only top 6 languages are considered. Hence, russian,japanese and polish tuples have been dropped from the dataframe.\n",
    "* Stemming is a crude rule-based method of converting different words to its root or base form. Snowball Stemmer from NLTK supports many languages.\n",
    "* Before stemming, the input text is tokenized, converted to lower case and then stemmed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "email_df.head()\n",
    "indices = email_df[(email_df.Language=='ru')|(email_df.Language=='ja')|(email_df.Language=='pl')].index\n",
    "email_df.drop(indices,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "en_stemmer = SnowballStemmer('english')\n",
    "fr_stemmer = SnowballStemmer('french')\n",
    "de_stemmer = SnowballStemmer('german')\n",
    "it_stemmer = SnowballStemmer('italian')\n",
    "es_stemmer = SnowballStemmer('spanish')\n",
    "nl_stemmer = SnowballStemmer('dutch')\n",
    "stemmers = {'en':en_stemmer,'fr':fr_stemmer,'de':de_stemmer,'it':it_stemmer,'es':es_stemmer,'nl':nl_stemmer}\n",
    "\n",
    "def stem_text(row):\n",
    "    ''' Stems text based with snowball stemmer based on the language.'''\n",
    "    \n",
    "    lang = row['Language']\n",
    "    text = row['Cleaned Emails']\n",
    "    text = ''.join([x.lower() for x in text])\n",
    "    #print(text)\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    #print(tokens)\n",
    "    stemmer = stemmers[lang]\n",
    "    #print(stemmer)\n",
    "    stemmed_text = ' '.join([stemmer.stem(token) for token in tokens])\n",
    "    \n",
    "    return stemmed_text\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "email_df['Cleaned Emails'] = email_df.apply(stem_text,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"grazi mill per aver trovato il tempo per mettervi in contatto con noi per questo problema ! purtroppo non parlo l'italiano , quindi spero vada bene lo stesso se rispondo in ingles : thank you so much for reach out and take the time to contact us about this issu ! usual , the notif alert on the app icon indic that there are new flyer or catalogu avail in your `` offer '' section in and should clear as soon as all notif that are tag as `` new '' have been open onc . unfortun , the notif alert on the app icon not disappear even though all new offer in have alreadi been open seem to be a bug on veri few devic at the moment . i can assur you that our develop are alreadi awar of the problem and tri to solv it as soon as possibl . altern , you can also disabl the notif badg complet in your general system set under `` notif '' - > `` `` - > `` badg app icon '' . in the meantim , we sincer apolog for the inconveni this caus and hope that you can use in it full function again soon . se dovest aver ulteriori domand , propost per miglioramenti o commenti generali , non esit a contattarmi di nuovo .\""
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "email_df['Cleaned Emails'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text Normalisation\n",
    "* Cucco is used for normalising the text since it supports multiple languages.\n",
    "* Text can be normalised based on a number of rules that we can provide like- removing stop words, punctuation, whitespace etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cucco\n",
    "from cucco import Cucco\n",
    "\n",
    "norm_en = Cucco(language='en')\n",
    "norm_es = Cucco(language='es')\n",
    "norm_fr = Cucco(language='fr')\n",
    "norm_it = Cucco(language='it')\n",
    "norm_de = Cucco(language='de')\n",
    "norm_nl = Cucco(language='nl')\n",
    "\n",
    "normalisers = {'en':norm_en,'es':norm_es,'fr':norm_fr,'it':norm_it,'nl':norm_nl,'de':norm_de}\n",
    "\n",
    "def normalise(row):\n",
    "    ''' Performs text normalisation for multiple languages. Removes stopwords,punctuation etc.'''\n",
    "    \n",
    "    lang = row['Language']\n",
    "    text = row['Cleaned Emails']\n",
    "    sents = nltk.sent_tokenize(text)\n",
    "    normaliser = normalisers[lang]\n",
    "    rules = ['remove_stop_words', 'replace_punctuation', 'remove_extra_whitespaces']\n",
    "    norm_text = ' '.join([normaliser.normalize(sent,rules) for sent in sents])\n",
    "    \n",
    "    return norm_text\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "email_df['Cleaned Emails'] = email_df.apply(normalise,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'merc votr messag feedback somm tres heureux ’ entendr plaît travaillon jour jour amélior développ notr appliqu utilis afin ’ ultim rendr portefeuill physiqu obsolèten ’ hésit partag ’ appliqu amis'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "email_df[email_df.Language=='fr']['Cleaned Emails'][10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dank fur nachricht dafur dass zeit genomm ruckmeld geb durft frag zugang alt handy kart geratewechsel neu handy ubertrag konn zuvor backup kart erstell welch neu handy wiederherstell konn dafur gibt backup funktion innerhalb app erlaubt kart cloud weit mobil gerat ubertrag funktioniert folgendermassen1 bitt geh einstell wahl backup android backup ios 2 konn entwed facebook googl privat email adress einlogg 3 kart gespeichert sobald erfolgreich angemeldet ios konn danach backup erstell geh android 1 bitt geh wied uber einstell backup neu handy2 meld denselb dat alt gerat beispielsweis googl benutzt bitt wied uber googl anmeld uber email adress 3 sobald angemeldet werd kart automat wiederhergestellt offnet fenst option entwed neu backup anzuleg alt backup wiederherzustell bitt wahl zweit option backup wiederherstell danach sollt all kart automat neu handy verfug stehenbitt beacht dass nutz problem hatt kart uber facebook wiederherzustell empfehl dah entwed googl email adress verwend probl facebook behob weit wurd rat darauf acht dass alt handy backup account abmeld bevor neu handy selb account einlogg wurd ausserd empfehl ab sofort backup nutz kart wiederherstell konn fur fall dass app deinstalli smartphon wechseln hoff konnt weiterhelf fall funktioniert weit frag konn gern jederzeit erneut kontakti'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "email_df[email_df.Language=='de']['Cleaned Emails'][17]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA\n",
    "* Latent Dirichlet Allocation.\n",
    "* Implemented using sci-kit learn and performed only on english data.\n",
    "* 5 topics have been considered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "eng_df = pd.read_pickle('./eng_df')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<342x973 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 17793 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv = CountVectorizer(max_df=0.95,min_df=2,stop_words='english')\n",
    "term_matrix = cv.fit_transform(eng_df['Cleaned Emails'])\n",
    "term_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\kushal\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\decomposition\\online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(batch_size=128, doc_topic_prior=None,\n",
       "             evaluate_every=-1, learning_decay=0.7, learning_method=None,\n",
       "             learning_offset=10.0, max_doc_update_iter=100, max_iter=10,\n",
       "             mean_change_tol=0.001, n_components=5, n_jobs=1,\n",
       "             n_topics=None, perp_tol=0.1, random_state=None,\n",
       "             topic_word_prior=None, total_samples=1000000.0, verbose=0)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda = LatentDirichletAllocation(n_components=5)\n",
    "lda.fit(term_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 973)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lda.components_)\n",
    "lda.components_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.22163034, 1.55830537, 0.22256588, ..., 0.22248275, 0.21900539,\n",
       "        0.22310291],\n",
       "       [0.22022823, 0.22307812, 2.99329219, ..., 0.22211049, 0.21790258,\n",
       "        0.21554685],\n",
       "       [0.22203806, 0.2209804 , 0.22090522, ..., 0.22775048, 0.22432041,\n",
       "        0.21888648],\n",
       "       [0.27438229, 0.27474994, 0.27122582, ..., 1.89427068, 1.88765363,\n",
       "        1.89585393],\n",
       "       [3.56592575, 1.08621551, 1.37921427, ..., 0.2245659 , 0.2186602 ,\n",
       "        0.22401345]])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "973"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lda.components_[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feedback\n",
      "issue\n",
      "don\n",
      "time\n",
      "taking\n",
      "scanners\n",
      "thank\n",
      "card\n",
      "scanning\n",
      "contact\n"
     ]
    }
   ],
   "source": [
    "topic = lda.components_[0]\n",
    "top_words_indices = topic.argsort()[-10:]\n",
    "for index in top_words_indices:\n",
    "    print(cv.get_feature_names()[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top words for topic 0\n",
      "['feedback', 'issue', 'don', 'time', 'taking', 'scanners', 'thank', 'card', 'scanning', 'contact']\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Top words for topic 1\n",
      "['restore', 'contact', 'facebook', 'device', 'mail', 'address', 'cards', 'google', 'account', 'backup']\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Top words for topic 2\n",
      "['attention', 'caused', 'stores', 'tesco', 'loyalty', 'thank', 'cards', 'digital', 'information', 'acceptance']\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Top words for topic 3\n",
      "['suggestions', 'don', 'questions', 'time', 'thank', 'contact', 'app', 'feedback', 'cards', 'card']\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Top words for topic 4\n",
      "['code', 'pin', 'notifications', 'access', 'app', 'lock', 'contact', 'settings', 'touch', 'id']\n",
      "------------------------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "topic_word_dict = {}\n",
    "for index,topic in enumerate(lda.components_):\n",
    "    words = [cv.get_feature_names()[i] for i in topic.argsort()[-10:]]\n",
    "    topic_word_dict[index] = words\n",
    "    print('Top words for topic {}'.format(index))\n",
    "    print(words)\n",
    "    print('-'*120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = lda.transform(term_matrix)\n",
    "eng_df['topic'] = topics.argmax(axis=1)\n",
    "\n",
    "def assign_topics(row):\n",
    "    topic = row['topic']\n",
    "    words = topic_word_dict[topic]\n",
    "    \n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_df['topic words'] = eng_df.apply(assign_topics,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Cleaned Emails</th>\n",
       "      <th>lang</th>\n",
       "      <th>topic</th>\n",
       "      <th>topic words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Merci pour votre message! Malheureusement, mon...</td>\n",
       "      <td>en</td>\n",
       "      <td>1</td>\n",
       "      <td>[restore, contact, facebook, device, mail, add...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>この問題についてご連絡いただきありがとうございます。 申し訳ありませんが私は日本語語が話せま...</td>\n",
       "      <td>en</td>\n",
       "      <td>3</td>\n",
       "      <td>[suggestions, don, questions, time, thank, con...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Grazie mille per aver trovato il tempo per met...</td>\n",
       "      <td>en</td>\n",
       "      <td>4</td>\n",
       "      <td>[code, pin, notifications, access, app, lock, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Thank you so much for reaching out and taking ...</td>\n",
       "      <td>en</td>\n",
       "      <td>3</td>\n",
       "      <td>[suggestions, don, questions, time, thank, con...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Thank you so much for reaching out and taking ...</td>\n",
       "      <td>en</td>\n",
       "      <td>4</td>\n",
       "      <td>[code, pin, notifications, access, app, lock, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      Cleaned Emails lang  topic  \\\n",
       "0  Merci pour votre message! Malheureusement, mon...   en      1   \n",
       "2  この問題についてご連絡いただきありがとうございます。 申し訳ありませんが私は日本語語が話せま...   en      3   \n",
       "3  Grazie mille per aver trovato il tempo per met...   en      4   \n",
       "4  Thank you so much for reaching out and taking ...   en      3   \n",
       "5  Thank you so much for reaching out and taking ...   en      4   \n",
       "\n",
       "                                         topic words  \n",
       "0  [restore, contact, facebook, device, mail, add...  \n",
       "2  [suggestions, don, questions, time, thank, con...  \n",
       "3  [code, pin, notifications, access, app, lock, ...  \n",
       "4  [suggestions, don, questions, time, thank, con...  \n",
       "5  [code, pin, notifications, access, app, lock, ...  "
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eng_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thank you so much for reaching out and taking the time to contact us about this issue! Please excuse the delayed response. I'm happy to inform you that you can already enlarge the front and back pictures of your cards simply by tapping on it once. Your card pictures will then get enlarged as well as rotated. However, I will also suggest to our developers to make zooming already in the \"Notes\" tab possible for future versions of  . I hope I was able to help you. If you have any further questions, suggestions for improvements or general feedback, please don't hesitate to contact me again.\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "3\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "['suggestions', 'don', 'questions', 'time', 'thank', 'contact', 'app', 'feedback', 'cards', 'card']\n",
      "------------------------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(eng_df['Cleaned Emails'][4])\n",
    "print('-'*120)\n",
    "print(eng_df['topic'][4])\n",
    "print('-'*120)\n",
    "print(topic_word_dict[eng_df['topic'][4]])\n",
    "print('-'*120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merci pour votre message! Malheureusement, mon français n'est pas si bon. J'espère que ça ne vous dérange pas, mais je vais devoir poursuivre en anglais :Thank you so much for reaching out and taking the time to contact us about this issue - I'm happy to help you with this!There is a feature in   that allows you to save your cards and transfer them to a second mobile device - the   Backup. You can create or restore a backup of your cards the following way:1. Go to the \"Settings\" tab in   and choose \"  Backup\" (Android) or \"Backup\" (iOS).2. Sign in via Facebook, Google or sign up using a private mail address.3. As soon as you are logged in you have successfully created a backup (iOS) or you can click on \"Backup Now\" to create your backup (Android).1. Go to the \"Settings\" tab in   and choose \"Backup\" again on your new device.2. Sign in with the same details you used when you created the account (e.g. when you used Google, you have to log in via Google again and not via mail address).3. After logging in your cards are automatically restored or a pop-up message appears with the options to either \"Restore old Backup\" or \"Create new Backup\". Please select \"Restore old Backup\" and then all your cards should automatically be available again on your new device.Please note that some users experienced problems with retrieving their cards via Facebook, so I suggest using either Google or a mail address until the problem is solved. Moreover, I would advise you to make sure that you are always only logged into the same   Backup account on one device at a time so that you don't overwrite your backup by mistake.I hope I was able to help you. However, if you have any further questions or need further assistance with the   Backup feature, please don't hesitate to contact me again.\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "1\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "['restore', 'contact', 'facebook', 'device', 'mail', 'address', 'cards', 'google', 'account', 'backup']\n",
      "------------------------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(eng_df['Cleaned Emails'][0])\n",
    "print('-'*120)\n",
    "print(eng_df['topic'][0])\n",
    "print('-'*120)\n",
    "print(topic_word_dict[eng_df['topic'][0]])\n",
    "print('-'*120)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Other information\n",
    "* The information omitted while removing salutations can be used to get some information, maybe the people involved in the email. This is just a crude attempt to extract info from minimal data.\n",
    "* Named-Entity Recognition can be performed on the text to get all the proper nouns used in the salutations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "info = list(email_df['Other Info'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Tanti saluti,',\n",
       " 'Isabelle van Capelleveen',\n",
       " 'Customer Support',\n",
       " '  GmbH',\n",
       " 'C-HUB / Hafenstraße 25-27']"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info[0].split('---')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Merci encore et une bonne fin de journée. ,Isabelle van Capelleveen ,Customer Support ,  GmbH ,C-HUB / Hafenstraße 25-27'"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = info[10].split('---')\n",
    "text = ' ,'.join([x for x in text])\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merci                         ORG                           Companies, agencies, institutions, etc.                     \n",
      "Isabelle van Capelleveen      PERSON                        People, including fictional                                 \n",
      "Customer Support              PERSON                        People, including fictional                                 \n",
      "C-HUB / Hafenstraße           ORG                           Companies, agencies, institutions, etc.                     \n",
      "25-27                         CARDINAL                      Numerals that do not fall under another type                \n"
     ]
    }
   ],
   "source": [
    "doc = nlp(text)\n",
    "\n",
    "ents = []\n",
    "if doc.ents:\n",
    "    for ent in doc.ents:\n",
    "        ents.append(ent.text)\n",
    "        print(f'{ent.text:{30}}{ent.label_:{30}}{spacy.explain(ent.label_):{60}}')\n",
    "else:\n",
    "    print('No entities')\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The information is not totally accurate but still provides some basic info.\n",
    "* Summarization has been  performed in different notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
